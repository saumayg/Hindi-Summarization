{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c191c8-43a8-476b-9206-02146058e839",
   "metadata": {},
   "source": [
    "# Google Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1b5512-ef0f-4b38-a0d5-05583079a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def extract_website(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    actual_url = query_params.get('url', [None])[0]\n",
    "    return actual_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db54fb8-cdba-4569-a5c9-f135be1ef339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import tldextract\n",
    "import pdb\n",
    "\n",
    "def google_crawler(query, num=10, domain_exclude=[], currentPage = 0):\n",
    "    results = dict()\n",
    "    headers = {\n",
    "        'User-agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582'\n",
    "    }    \n",
    "    query = query.replace(' ', '+')\n",
    "    while True:\n",
    "        url = f\"https://www.google.com/search?q={query}&start={currentPage}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            print(\"Stopping due to 429 code\")\n",
    "            return [429], 0\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        search_items = soup.find_all('div', class_=['egMi0 kCrYT', 'Gx5Zad fP1Qef xpd EtOod pkphOe'])\n",
    "        if not search_items:\n",
    "            break\n",
    "        for items in search_items:\n",
    "            link = items.find('a', href=True)\n",
    "            if link:\n",
    "                link = link['href']\n",
    "                link = extract_website(link)\n",
    "                if link and '.pdf' not in link:\n",
    "                    domain = tldextract.extract(link).domain\n",
    "                    if domain not in domain_exclude:\n",
    "                        results[link] = \"\"\n",
    "        currentPage += 10\n",
    "        time.sleep(1)\n",
    "        if len(results) >= num:\n",
    "            break\n",
    "\n",
    "    return list(results.keys())[:num], currentPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f7e6c1-0db4-4c43-bf71-c8712e0e80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from requests.exceptions import RequestException, Timeout\n",
    "from urllib3.exceptions import LocationParseError\n",
    "\n",
    "timeout_seconds = 30\n",
    "\n",
    "def extract_text_and_detect_language(url):\n",
    "    # response_head = requests.head(url)\n",
    "    # if response_head.headers.get('Content-Type') == 'application/pdf':\n",
    "    #     print(f\"{url} points to a PDF file. Skipping...\")\n",
    "    #     return '', ''\n",
    "    # Fetch the content of the URL\n",
    "    try:\n",
    "        # print(f\"Trying {url}\")\n",
    "        response = requests.get(url, timeout=timeout_seconds)\n",
    "        # print(f\"Got {url}\")\n",
    "    except Timeout:\n",
    "        # print(f\"Request timed out for {url}\")\n",
    "        return '', ''\n",
    "    except RequestException as e:\n",
    "        return '', ''\n",
    "    except LocationParseError as e:\n",
    "        print(f\"Location parse error for {url}\")\n",
    "        return '', ''\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse the HTML content and extract text\n",
    "\n",
    "    # print(f\"Parsing {url}\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # print(f\"Parsed {url}\")\n",
    "    # Extracting all text from paragraphs. This is a basic approach, and you might need to adjust it based on the website's structure.\n",
    "    for unwanted in soup(['header', 'footer', 'nav', 'img', 'figcaption', 'h1', 'h2',]):\n",
    "        unwanted.decompose()  # Remove these elements from the soup\n",
    "    \n",
    "    # Assuming the main content is within 'article' or 'main'. Adjust as needed.\n",
    "    main_content = soup.find(class_=[\"printableindent\", \"content\", \"articlecontent\", \"story-content\", \"post-content\", \"page\", \"abp-story-detail\", \"mw-body\", \"text-formatted\", \"entry-content\", \"khbr_rght_sec\", \"innner-page-main-about-us-content-right-part\", \"article-desc ul_styling\"])\n",
    "\n",
    "    if not main_content:\n",
    "        main_content = soup.find(['main', 'article'])\n",
    "    text = main_content.get_text(separator=' ', strip=True) if main_content else ''\n",
    "\n",
    "    \n",
    "    \n",
    "    # Detect the language of the extracted text\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except Exception as e:\n",
    "        language = \"Language detection failed\"\n",
    "    \n",
    "    return text, language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff89c73-7e68-4e52-b7db-32ba4fc86ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(df, idx):\n",
    "    types = {'LOCATION': 3, 'PERSON': 3, 'ORGANIZATION': 1, 'TIMEX': 1}\n",
    "    ent_list = list()\n",
    "    sent = ''\n",
    "    rec = df[idx]\n",
    "    for ent in rec[\"all_list\"]:\n",
    "        if ent['entity_group'] in types.keys() and types[ent['entity_group']] > 0:\n",
    "            if ent['word'] not in ent_list:\n",
    "                ent_list.append(ent['word'])\n",
    "                types[ent['entity_group']] -= 1\n",
    "                sent = sent + ent['word'] + ' '\n",
    "    return sent\n",
    "\n",
    "def get_query_2(df, idx):\n",
    "    types = {'LOCATION': 2, 'PERSON': 2, 'ORGANIZATION': 1, 'TIMEX': 1}\n",
    "    ent_list = list()\n",
    "    sent = ''\n",
    "    rec = df[idx]\n",
    "    for ent in rec[\"all_list\"]:\n",
    "        if ent['entity_group'] in types.keys() and types[ent['entity_group']] > 0:\n",
    "            if ent['word'] not in ent_list:\n",
    "                ent_list.append(ent['word'])\n",
    "                types[ent['entity_group']] -= 1\n",
    "                sent = sent + ent['word'] + ' '\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "634f8a12-bcd6-4ab7-9f15-6a076eb01e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "def get_embedding(model, tokenizer, text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    except:\n",
    "        print(text)\n",
    "        pdb.set_trace()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "def check_link_valid(link):\n",
    "    text, lang = extract_text_and_detect_language(link)\n",
    "    if lang != 'hi' or text == '' or len(text.split(' ')) < 10:\n",
    "        return False, '', ''\n",
    "\n",
    "    return True, text, link\n",
    "\n",
    "def get_score_and_valid_text(text_dict, text_orig, thresh):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    link_list = list(text_dict.keys())\n",
    "    text_list = list(text_dict.values())\n",
    "    \n",
    "    embedding1 = get_embedding(model, tokenizer, text_list)\n",
    "    embedding2 = get_embedding(model, tokenizer, text_orig)\n",
    "\n",
    "    final_list = []\n",
    "    for i in range(embedding1.shape[0]):\n",
    "        cos_sim = 1 - cosine(embedding1[i].numpy(), embedding2[0].numpy())\n",
    "        if (cos_sim >= thresh):\n",
    "            final_list.append({\n",
    "                \"link\": link_list[i],\n",
    "                \"text\": text_list[i],\n",
    "                \"score\": cos_sim\n",
    "            })\n",
    "            \n",
    "    return final_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b74a340-96d4-4651-b3e2-507dfaaec58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ner = pd.read_json(\"../Datasets/Hindi_summarization/Long-short-news-dataset/NER/ner_hindi_train_1024.json\")\n",
    "df = pd.read_csv(\"../Datasets/Hindi_summarization/Long-short-news-dataset/clean_hindi_train.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc75158f-7623-4f5e-928d-7e2bc79873e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_links(res):\n",
    "    text_dict = dict()\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_link = [executor.submit(check_link_valid, link) for link in res]\n",
    "\n",
    "        for future in as_completed(future_to_link):\n",
    "            check, link_text, link = future.result()\n",
    "            if check:\n",
    "                text_dict[link] = link_text\n",
    "\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3abd41e1-24e7-4c15-82c0-eda2eeb3956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "def process_index(idx, df_ner, df, exclude, cos_sim_thresh):\n",
    "    # print(idx)\n",
    "    query = get_query(df_ner, idx)\n",
    "    # print(\"Query\")\n",
    "    nextPage = 0\n",
    "    final_list = []\n",
    "    res, nextPage = google_crawler(query, 10, exclude, nextPage)\n",
    "    # print(\"res\")\n",
    "    if len(res) == 1 and res[0] == 429:\n",
    "        return idx, None, None\n",
    "    # print(res)\n",
    "    text_dict = process_links(res)\n",
    "    if len(text_dict) > 0:\n",
    "        final_list = get_score_and_valid_text(text_dict, df['article'][idx], cos_sim_thresh)\n",
    "        final_list = sorted(final_list, key = lambda item: item['score'], reverse = True)\n",
    "        \n",
    "    if (len(final_list) == 0):\n",
    "        print(f\"Trying with smaller query.\")\n",
    "        query = get_query_2(df_ner, idx)\n",
    "        nextPage = 0\n",
    "        final_list = []\n",
    "        res, nextPage = google_crawler(query, 10, exclude, nextPage)\n",
    "        if len(res) == 1 and res[0] == 429:\n",
    "            return idx, None\n",
    "        text_dict = process_links(res)\n",
    "        if len(text_dict) > 0:\n",
    "            final_list = get_score_and_valid_text(text_dict, df['article'][idx], cos_sim_thresh)\n",
    "            final_list = sorted(final_list, key = lambda item: item['score'], reverse = True)\n",
    "        \n",
    "    return df['Id'][idx], final_list[:10], query\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def save_partial_results(data, save_doc):\n",
    "    if os.path.exists(save_doc):\n",
    "        with open(save_doc, 'a') as file:\n",
    "            for key in list(data.keys()):\n",
    "                record = {'key': key, 'data': data[key]}\n",
    "                json_record = json.dumps(record, cls = NpEncoder)\n",
    "                file.write(json_record + '\\n')\n",
    "    else:\n",
    "        with open(save_doc, 'w') as file:\n",
    "            for key in list(data.keys()):\n",
    "                record = {'key': key, 'data': data[key]}\n",
    "                json_record = json.dumps(record, cls = NpEncoder)\n",
    "                file.write(json_record + '\\n')\n",
    "\n",
    "def get_saved_keys(save_doc):\n",
    "    existing_keys = set()\n",
    "    if os.path.exists(save_doc):\n",
    "        with open(save_doc, 'r') as file:\n",
    "            for line in file:\n",
    "                existing_keys.add(json.loads(line)['key'])\n",
    "\n",
    "    return existing_keys\n",
    "\n",
    "def main(df, df_ner, exclude, cos_sim_thresh, save_doc):\n",
    "    saved_keys = get_saved_keys(save_doc)\n",
    "    data = {}\n",
    "    cnt = 0\n",
    "    for idx in tqdm(df.index):\n",
    "        if df['Id'][idx] in saved_keys:\n",
    "            continue\n",
    "        # print(f\"Working on {idx}\")\n",
    "        id, final_list, query = process_index(idx, df_ner, df, exclude, cos_sim_thresh)\n",
    "        if final_list == None:\n",
    "            break\n",
    "        print(f\"{idx} has {len(final_list)}\")\n",
    "        if (len(final_list) == 0):\n",
    "            # print(f\"{idx} has 0 len\")\n",
    "            cnt += 1\n",
    "        data[id] = final_list\n",
    "\n",
    "        # Optionally save after every 100 completions\n",
    "        if len(data) % 20 == 0:\n",
    "            save_partial_results(data, save_doc)\n",
    "            data = {}\n",
    "\n",
    "        # pdb.set_trace()\n",
    "\n",
    "    # Final save\n",
    "    save_partial_results(data, save_doc)\n",
    "    print(f\"{cnt} empty out of {len(df.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "111c38a1-b958-4e9a-98e9-6dfaa9045267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {idx: [{link: , text: ,score: }]}\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "exclude = [\"youtube\", \"instagram\", \"facebook\", \"twitter\"]\n",
    "num_docs = 10\n",
    "cos_sim_thresh = 0.8\n",
    "\n",
    "save_doc = \"LSN-train-crawl.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cece2cd-6738-48fe-98c0-9fd741d5bf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 58084/58084 [00:00<00:00, 102557.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra: set()\n",
      "Duplicate: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_extra_keys(save_doc, df):\n",
    "    existing_keys = set()\n",
    "    duplicate = set()\n",
    "    if os.path.exists(save_doc):\n",
    "        with open(save_doc, 'r') as file:\n",
    "            for line in file:\n",
    "                key = json.loads(line)['key']\n",
    "                if key in existing_keys:\n",
    "                    duplicate.add(key)\n",
    "                existing_keys.add(json.loads(line)['key'])\n",
    "\n",
    "    for idx in tqdm(df.index):\n",
    "        if df['Id'][idx] in existing_keys:\n",
    "            existing_keys.remove(df['Id'][idx])\n",
    "\n",
    "    return existing_keys, duplicate\n",
    "\n",
    "extra_keys, duplicate = get_extra_keys(save_doc, df)\n",
    "print(f\"Extra: {extra_keys}\")\n",
    "print(f\"Duplicate: {duplicate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe351a2-d0d2-41f3-8111-4eb80ab82186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call main function with appropriate arguments\n",
    "main(df, df_ner, exclude, cos_sim_thresh, save_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2987ec-1e19-426b-b060-5935d1a7de46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71ad6705",
   "metadata": {},
   "source": [
    "# Removing same articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdc787-34d1-4639-9259-599bb0c6c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_partial_results(key, data, save_doc):\n",
    "    if os.path.exists(save_doc):\n",
    "        with open(save_doc, 'a') as file:\n",
    "            record = {'key': key, 'data': data}\n",
    "            json_record = json.dumps(record)\n",
    "            file.write(json_record + '\\n')\n",
    "    else:\n",
    "        with open(save_doc, 'w') as file:\n",
    "            record = {'key': key, 'data': data}\n",
    "            json_record = json.dumps(record)\n",
    "            file.write(json_record + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e82216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/Hindi_summarization/Long-short-news-dataset/clean_hindi_test.csv\", lineterminator='\\n')\n",
    "df.set_index('Id', inplace = True)\n",
    "save_doc = \"LSN-test-crawl-clear.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with open(\"../Datasets/Hindi_summarization/Long-short-news-dataset/LSN-test-crawl.jsonl\", 'rb') as file:\n",
    "    for line in tqdm(file):\n",
    "        rec = json.loads(line)\n",
    "        key = rec['key']\n",
    "        data= rec['data']\n",
    "\n",
    "        text = df.loc[key, 'article']\n",
    "        text_tokens = set(indic_tokenize.trivial_tokenize(text))\n",
    "        if len(data) == 0:\n",
    "            save_partial_results(key, data, save_doc)\n",
    "            continue\n",
    "\n",
    "        new_article_list = []\n",
    "        for article in data:\n",
    "            article_tokens = set(indic_tokenize.trivial_tokenize(article['text']))\n",
    "            common = article_tokens.intersection(text_tokens)\n",
    "            percent = (len(common)/len(text_tokens))*100\n",
    "\n",
    "            if percent < 90:\n",
    "                new_article_list.append(article)\n",
    "\n",
    "        save_partial_results(key, new_article_list, save_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d74181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_extra_keys(save_doc, df):\n",
    "    existing_keys = set()\n",
    "    duplicate = set()\n",
    "    if os.path.exists(save_doc):\n",
    "        with open(save_doc, 'r') as file:\n",
    "            for line in file:\n",
    "                key = json.loads(line)['key']\n",
    "                if key in existing_keys:\n",
    "                    duplicate.add(key)\n",
    "                existing_keys.add(json.loads(line)['key'])\n",
    "\n",
    "    for idx in tqdm(df.index):\n",
    "        if df['Id'][idx] in existing_keys:\n",
    "            existing_keys.remove(df['Id'][idx])\n",
    "\n",
    "    return existing_keys, duplicate\n",
    "\n",
    "df = pd.read_csv(\"../Datasets/Hindi_summarization/Long-short-news-dataset/clean_hindi_test.csv\", lineterminator='\\n')\n",
    "extra_keys, duplicate = get_extra_keys(save_doc, df)\n",
    "print(f\"Extra: {extra_keys}\")\n",
    "print(f\"Duplicate: {duplicate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8097aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cnt = 0\n",
    "cnt2 = 0\n",
    "with open(save_doc, 'r') as input_file:\n",
    "    for i, line in enumerate(input_file):\n",
    "        rec = json.loads(line)\n",
    "        if len(rec['data']) == 0:\n",
    "          # print(i)\n",
    "          cnt += 1\n",
    "        cnt2 += 1\n",
    "\n",
    "print(f\"Empty: {cnt}\")\n",
    "print(f\"Total: {cnt2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
